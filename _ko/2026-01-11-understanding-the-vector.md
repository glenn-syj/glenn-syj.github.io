---
title: 벡터 이해하기
lang: ko
layout: post
---

## 들어가며

- 이 글은 LLM을 이해하기 위한 가장 작은 단위인 벡터(Vector)에서부터 시작하기 위해 작성되었다.
- 최종 수정일: 26/01/25

업무적으로나 개인적으로 LLM을 활용하고 공부하다 보면, 토큰이나 임베딩과 같은 개념을 자주 마주하게 된다. 간략하게 이해는 다음과 같다. 문서는 벡터로 변환되고, 질의는 벡터로 바뀌며, 모델은 벡터를 입력으로 받아 또 다른 벡터를 출력한다.

여기에서 벡터를 설명할 때는 흔히 "의미적 유사성을 수치로 표현한 것"이라는 말이 따라붙는다. 의미가 비슷한 단어일수록 벡터 사이의 거리가 가깝다는 식이다. 하지만 이런 설명은 벡터의 내부가 무엇인지, 그 숫자들이 어떤 구조를 이루고 있는지에 대해서는 거의 말해주지 않는다. 간략하게 최종 결과가 문장으로 변환되는 과정을 이해할 뿐, 그 사이에서 어떤 변환이 일어나는지는 쉽게 파악할 수 없다.

따라서 LLM을 이해하는 단계에서 한 발짝 더 나아가려면, 벡터라는 자료구조를 외면할 수 없다. 토큰, 임베딩, 히든 스테이트, 로짓 같은 용어들은 모두 벡터의 다른 얼굴일 뿐이다. 모델은 실제로는 텍스트가 아니라 벡터를 다룬다. 문장이나 의미는 직접 처리되지 않는다. 오직 고정된 차원의 실수 벡터들이 행렬 연산을 거치며 변형될 뿐이다.

이 글은 그 지점에서 출발하고자 한다. 단순히 벡터의 정의로 끝내기 보다는 왜 벡터라는 표현 형식을 선택했는지, 그리고 그 선택이 어떤 사고방식과 계산 구조를 가능하게 만들었는지를 따라가 보고자 한다.

## 벡터와 토큰

### 개념

- **토큰(Token)**

토큰은 현대의 신경망 기반 자연어 처리 모델이 텍스트를 처리하기 위해 입력받는 최소의 단위이다. 문자열은 토크나이저(Tokenizer)를 통해 특정 규칙에 따라 분절된다. 각 토큰은 모델이 미리 정의한 어휘 사전(Vocabulary) 내의 고유한 정수 인덱스(Integer Index)에 매핑된다. 모델이 받는 것은 문장이나 단어가 아니라, 토큰 ID의 시퀀스다.

- **벡터(Vector)**

벡터는 토큰 ID가 임베딩 테이블을 통해 변환된 결과로, 고정된 차원을 갖는 실수 배열이다. 이러한 신경망 기반 모델 내부에서 모든 정보는 밀집 벡터(Dense Vector)로 표현된다. 모델은 텍스트나 의미를 직접 이해하거나 다루지 않는다. 대신 벡터들 사이의 내적, 합성, 선형 변환과 같은 기하학적 연산만을 수행한다. 의미라고 여겨지는 것은 이 고차원 공간에서의 위치와 관계를 사후적으로 해석한 결과다.

### 관계

토큰과 벡터는 **이산적 기호**와 **연속적 연산 값** 사이를 잇는 관계에 놓여 있다. 토큰이 구분 가능한 표식에 머문다면, 벡터는 계산이 가능한 물리적 표현이다. 모델은 이 둘을 직접 넘나들지 않고, 참조를 통해서 기호를 연산의 세계로 옮긴다. 이 지점에서 세 가지 주요한 특징이 드러난다.

- **매핑 구조**

하나의 토큰 ID는 하나의 고정 차원 벡터와 일대일로 대응된다. 모델은 토큰 ID를 주소값처럼 사용해, 가중치 행렬(임베딩 테이블)에서 해당 벡터를 추출한다. 이 과정은 해석이 아니라 조회에 가깝다.

- **정보의 구체화**

토큰 ID는 그 자체로는 비교나 변형이 불가능한 기호에 불과하다. 그러나 벡터로 변환되는 순간부터 각 차원의 성분값을 통해 거리, 유사성, 방향과 같은 기하학적 연산이 가능해진다. 토큰이 벡터라는 계산 가능한 실체로 바뀌는 지점이다.

- **가변성**

입력 단계에서의 벡터는 정적이지만 레이어를 통과하며 주변 벡터들과 연산을 거치면 그 성분값은 계속 변화한다. 동일한 토큰이라도 이미 형성된 벡터들의 상태에 따라 서로 다른 좌표로 이동한다. 우리가 컨텍스트라고 부르는 것 역시, 동일한 출발점이 서로 다른 연산의 장 위에서 다른 경로를 따르며 형성된 벡터 상태의 총합에 가깝다.

### 임베딩

임베딩은 이산적인 토큰을 고차원 연속 공간의 벡터로 전환하는 과정이자 그 결과물이다. 토큰은 의미로 해석되는 것이 아니라, 모델 내부의 좌표계에 배치된다. 각 토큰은 임베딩 행렬에서 하나의 벡터를 할당받고 연속적인 공간 위에 놓인다. 이 공간에서는 거리, 방향, 조합과 같은 기하학적 연산이 가능해지며, 이후의 모든 계산은 이 좌표 위에서 이루어진다.

벡터의 각 성분값 자체로는 인간이 해석할 수 있는 축으로 나뉘어 있지 않다는 점도 유의해야 한다. 임베딩 공간은 의미의 사전이 아니라, 연산이 가장 잘 작동하도록 형성된 공간에 가깝다. 유사성이나 단어 간 관계는 이를 인간의 언어로 받아들이는 과정의 것이다.

여기에서 좌표계는 실제로 모델 내부에서 구체적인 파라미터 행렬로 구현된다. 그 성질은 다음 세 가지로 정리할 수 있다.

- **구현 구조: 임베딩 행렬(Embedding Matrix)**

$V \times d$ 크기의 파라미터 행렬이다 ($V$: 어휘 사전 크기, $d$: 모델의 은닉층 차원). 토큰 ID가 $i$일 때, 임베딩 행렬의 $i$번째 행이 해당 토큰의 초기 벡터가 된다.

- **표현 형식: 밀집 표현(Dense Representation)**

1과 0으로만 표현되는 원-핫 인코딩(One-hot Encoding)과 달리, 수백~수천 차원의 공간 안에 실수값을 빽빽하게 채워 넣는다. 이는 데이터의 특징을 압축하여 표현하며 계산 효율성을 극대화한다.

- **형성 방식: 학습 가능한 파라미터(Learnable Parameters)**

임베딩 벡터 내의 각 실수값들은 고정된 것이 아니라, 모델 학습 과정에서 손실 함수(Loss Function)를 최소화하는 방향으로 최적화된다. 즉, 모델은 학습을 통해 각 토큰이 벡터 공간의 어느 좌표에 위치해야 연산에 유리한지를 스스로 결정한다.
